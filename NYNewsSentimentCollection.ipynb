{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sys, csv, json\n",
    "reload(sys)\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below part is not required\n",
    "########## News API ########################################################\n",
    "import newsapi\n",
    "\n",
    " #key = ('96af62a035db45bda517a9ca62a25ac3')\n",
    " #params = {}\n",
    " #api = NewsAPI(key)\n",
    " #sources = api.sources(params)\n",
    " #articles = api.articles(sources[0]['id'], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIKeyException(Exception):\n",
    "    def __init__(self, message): self.message = message \n",
    "\n",
    "class InvalidQueryException(Exception):\n",
    "    def __init__(self, message): self.message = message \n",
    "\n",
    "class ArchiveAPI(object):\n",
    "    def __init__(self, key):\n",
    "        \"\"\"\n",
    "        Initializes the ArchiveAPI class. Raises an exception if no API key is given.\n",
    "        :param key: New York Times API Key\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        #https://api.nytimes.com/svc/search/v2/articlesearch.json?q=election&api-key=yourkey\n",
    "        self.root = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key={}' \n",
    "        if not self.key:\n",
    "            nyt_dev_page = 'http://developer.nytimes.com/docs/reference/keys'\n",
    "            exception_str = 'Warning: API Key required. Please visit {}'\n",
    "            raise NoAPIKeyException(exception_str.format(nyt_dev_page))\n",
    "\n",
    "    def query(self, year=None, month=None,  key=None,):\n",
    "        \"\"\"\n",
    "        Calls the archive API and returns the results as a dictionary.\n",
    "        :param key: Defaults to the API key used to initialize the ArchiveAPI class.\n",
    "        \"\"\"\n",
    "        if not key: key = self.key\n",
    "        if (year < 1882) or not (0 < month < 13):\n",
    "            # currently the Archive API only supports year >= 1882\n",
    "            exception_str = 'Invalid query: See http://developer.nytimes.com/archive_api.json'\n",
    "            raise InvalidQueryException(exception_str)\n",
    "        url = self.root.format(year, month, key)\n",
    "        r = requests.get(url)\n",
    "        return r.json()\n",
    "\n",
    "# Replace below key with your NYTimes Developer key\n",
    "api = ArchiveAPI('YzUwoSmqiVR02u9SSoaX1MNO9YdZwDXa')\n",
    "\n",
    "years = [2016, 2015, 2014, 2013]\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        mydict = api.query(year, month)\n",
    "        file_str = '/Users/Ujash/Documents/StockPredictionJPM/NewsData/' + str(year) + '-' + '{:02}'.format(month) + '.json'\n",
    "        with open(file_str, 'w') as fout:\n",
    "            json.dump(mydict, fout)\n",
    "        fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "FNAME = \"snp500_formatted.txt\"\n",
    "stocks = []\n",
    "\n",
    "def getNewsForDate(date):\n",
    "    file = open('data/news/' + date.strftime('%Y-%m-%d') + '.csv', 'w')\n",
    "    print('Getting news for ' + date.strftime('%Y-%m-%d'))\n",
    "    for i in range(len(stocks)):\n",
    "        query = 'http://www.reuters.com/finance/stocks/companyNews?symbol=' + stocks[i] + '&date=' + format(date.month, '02d') + format(date.day, '02d') + str(date.year)\n",
    "        print('Getting news for ' + stocks[i])\n",
    "\n",
    "        response = requests.get(query)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        divs = soup.findAll('div', {'class': 'feature'})\n",
    "        print('Found ' + str(len(divs)) + ' articles.')\n",
    "\n",
    "        if(len(divs) == 0):\n",
    "            continue\n",
    "\n",
    "        data = u''\n",
    "        for div in divs:\n",
    "            data = data.join(div.findAll(text=True))\n",
    "        file.write(stocks[i] + ',' + data.encode('utf-8').replace('\\n', ' '))\n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "def getNews():\n",
    "    dataHistFile = open('dat.pkl', 'rb')\n",
    "    dataHist = pickle.load(dataHistFile)\n",
    "    date = dataHist['last_updated'] + datetime.timedelta(days=1)\n",
    "    endDate = datetime.date.today()\n",
    "\n",
    "    while(date <= endDate):\n",
    "        getNewsForDate(date)\n",
    "        date += datetime.timedelta(days=1)\n",
    "\n",
    "    dataHist['last_updated'] = endDate\n",
    "    dataHistFile.seek(0)\n",
    "    pickle.dump(dataHist, dataHistFile, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    dataHistFile.close()\n",
    "\n",
    "def init():\n",
    "    global stocks\n",
    "    with open(FNAME) as f:\n",
    "        stocks = f.readlines()\n",
    "    for i in range(len(stocks)):\n",
    "        stocks[i] = stocks[i].rstrip('\\n')\n",
    "\n",
    "    getNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('YzUwoSmqiVR02u9SSoaX1MNO9YdZwDXa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujash/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:153: DeprecationWarning: The SafeConfigParser class has been renamed to ConfigParser in Python 3.2. This alias will be removed in future versions. Use ConfigParser directly instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-144d933c4566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-144d933c4566>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSafeConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mscript_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mconfig_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'config/settings.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import sys, os\n",
    "import logging\n",
    "from configparser import SafeConfigParser\n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "\n",
    "\n",
    "# helper function to iterate through dates\n",
    "def daterange( start_date, end_date ):\n",
    "    if start_date <= end_date:\n",
    "        for n in range( ( end_date - start_date ).days + 1 ):\n",
    "            yield start_date + datetime.timedelta( n )\n",
    "    else:\n",
    "        for n in range( ( start_date - end_date ).days + 1 ):\n",
    "            yield start_date - datetime.timedelta( n )\n",
    "\n",
    "# helper function to get json into a form I can work with       \n",
    "def convert(input):\n",
    "    if isinstance(input, dict):\n",
    "        return {convert(key): convert(value) for key, value in input.iteritems()}\n",
    "    elif isinstance(input, list):\n",
    "        return [convert(element) for element in input]\n",
    "    elif isinstance(input, unicode):\n",
    "        return input.encode('utf-8')\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "# helpful function to figure out what to name individual JSON files        \n",
    "def getJsonFileName(date, page, json_file_path):\n",
    "    json_file_name = \".\".join([date,str(page),'json'])\n",
    "    json_file_name = \"\".join([json_file_path,json_file_name])\n",
    "    return json_file_name\n",
    "\n",
    "# helpful function for processing keywords, mostly    \n",
    "def getMultiples(items, key):\n",
    "    values_list = \"\"\n",
    "    if len(items) > 0:\n",
    "        num_keys = 0\n",
    "        for item in items:\n",
    "            if num_keys == 0:\n",
    "                values_list = item[key]                \n",
    "            else:\n",
    "                values_list =  \"; \".join([values_list,item[key]])\n",
    "            num_keys += 1\n",
    "    return values_list\n",
    "    \n",
    "# get the articles from the NYTimes Article API    \n",
    "def getArticles(date, query, api_key, json_file_path):\n",
    "    # LOOP THROUGH THE 101 PAGES NYTIMES ALLOWS FOR THAT DATE\n",
    "    for page in range(101):\n",
    "        for n in range(5): # 5 tries\n",
    "            try:\n",
    "                request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key\n",
    "                response = urllib2.urlopen(request_string)\n",
    "                content = response.read()\n",
    "                if content:\n",
    "                    articles = convert(json.loads(content))\n",
    "                    # if there are articles here\n",
    "                    if len(articles[\"response\"][\"docs\"]) >= 1:\n",
    "                        json_file_name = getJsonFileName(date, page, json_file_path)\n",
    "                        json_file = open(json_file_name, 'w')\n",
    "                        json_file.write(content)\n",
    "                        json_file.close()\n",
    "                    # if no more articles, go to next date\n",
    "                    else:\n",
    "                        return\n",
    "                time.sleep(3) # wait so we don't overwhelm the API\n",
    "            except HTTPError as e:\n",
    "                logging.error(\"HTTPError on page %s on %s (err no. %s: %s) Here's the URL of the call: %s\", page, date, e.code, e.reason, request_string)\n",
    "                if e.code == 403:\n",
    "                    print (\"Script hit a snag and got an HTTPError 403. Check your log file for more info.\")\n",
    "                    return\n",
    "                if e.code == 429:\n",
    "                    print (\"Waiting. You've probably reached an API limit.\")\n",
    "                    time.sleep(30) # wait 30 seconds and try again\n",
    "            except: \n",
    "                logging.error(\"Error on %s page %s: %s\", date, file_number, sys.exc_info()[0])\n",
    "                continue\n",
    "\n",
    "# parse the JSON files you stored into a tab-delimited file\n",
    "def parseArticles(date, tsv_file_name, json_file_path):\n",
    "\n",
    "    for file_number in range(101):\n",
    "        # get the articles and put them into a dictionary\n",
    "        try:\n",
    "            file_name = getJsonFileName(date,file_number, json_file_path)\n",
    "            if os.path.isfile(file_name):\n",
    "                in_file = open(file_name, 'r')\n",
    "                articles = convert(json.loads(in_file.read()))\n",
    "                in_file.close()\n",
    "            else:\n",
    "                break\n",
    "        except IOError as e:\n",
    "            logging.error(\"IOError in %s page %s: %s %s\", date, file_number, e.errno, e.strerror)\n",
    "            continue\n",
    "        \n",
    "        # if there are articles in that document, parse them\n",
    "        if len(articles[\"response\"][\"docs\"]) >= 1:  \n",
    "\n",
    "            # open the tsv for appending\n",
    "            try:\n",
    "                out_file = open(tsv_file_name, 'ab')\n",
    "\n",
    "            except IOError as e:\n",
    "                logging.error(\"IOError: %s %s %s %s\", date, file_number, e.errno, e.strerror)\n",
    "                continue\n",
    "        \n",
    "            # loop through the articles putting what we need in a tsv   \n",
    "            try:\n",
    "                for article in articles[\"response\"][\"docs\"]:\n",
    "                    # if (article[\"source\"] == \"The New York Times\" and article[\"document_type\"] == \"article\"):\n",
    "                    keywords = \"\"\n",
    "                    keywords = getMultiples(article[\"keywords\"],\"value\")\n",
    "    \n",
    "                    # should probably pull these if/else checks into a module\n",
    "                    variables = [\n",
    "                        article[\"pub_date\"], \n",
    "                        keywords, \n",
    "                        str(article[\"headline\"][\"main\"]).decode(\"utf8\").replace(\"\\n\",\"\") if \"main\" in article[\"headline\"].keys() else \"\", \n",
    "                        str(article[\"source\"]).decode(\"utf8\") if \"source\" in article.keys() else \"\", \n",
    "                        str(article[\"document_type\"]).decode(\"utf8\") if \"document_type\" in article.keys() else \"\", \n",
    "                        article[\"web_url\"] if \"web_url\" in article.keys() else \"\",\n",
    "                        str(article[\"news_desk\"]).decode(\"utf8\") if \"news_desk\" in article.keys() else \"\",\n",
    "                        str(article[\"section_name\"]).decode(\"utf8\") if \"section_name\" in article.keys() else \"\",\n",
    "                        str(article[\"snippet\"]).decode(\"utf8\").replace(\"\\n\",\"\") if \"snippet\" in article.keys() else \"\",\n",
    "                        str(article[\"lead_paragraph\"]).decode(\"utf8\").replace(\"\\n\",\"\") if \"lead_paragraph\" in article.keys() else \"\",\n",
    "                        ]\n",
    "                    line = \"\\t\".join(variables)\n",
    "                    out_file.write(line.encode(\"utf8\")+\"\\n\")\n",
    "            except KeyError as e:\n",
    "                logging.error(\"KeyError in %s page %s: %s %s\", date, file_number, e.errno, e.strerror)\n",
    "                continue\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except: \n",
    "                logging.error(\"Error on %s page %s: %s\", date, file_number, sys.exc_info()[0])\n",
    "                continue\n",
    "        \n",
    "            out_file.close()\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "# Main function where stuff gets done\n",
    "\n",
    "def main():\n",
    "    \n",
    "    config = SafeConfigParser()\n",
    "    script_dir = os.path.dirname(__file__)\n",
    "    config_file = os.path.join(script_dir, 'config/settings.cfg')\n",
    "    config.read(config_file)\n",
    "    \n",
    "    json_file_path = config.get('files','json_folder')\n",
    "    tsv_file_name = config.get('files','tsv_file')\n",
    "    log_file = config.get('files','logfile')\n",
    "    \n",
    "    api_key = config.get('nytimes','api_key')    \n",
    "    start = datetime.date( year = int(config.get('nytimes','start_year')), month = int(config.get('nytimes','start_month')), day = int(config.get('nytimes','start_day')) )\n",
    "    end = datetime.date( year = int(config.get('nytimes','end_year')), month = int(config.get('nytimes','end_month')), day = int(config.get('nytimes','end_day')) )\n",
    "    query = config.get('nytimes','query')\n",
    "        \n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO)\n",
    "    \n",
    "    logging.info(\"Getting started.\") \n",
    "    try:\n",
    "        # LOOP THROUGH THE SPECIFIED DATES\n",
    "        for date in daterange( start, end ):\n",
    "            date = date.strftime(\"%Y%m%d\")\n",
    "            logging.info(\"Working on %s.\" % date)\n",
    "            getArticles(date, query, api_key, json_file_path)\n",
    "            parseArticles(date, tsv_file_name, json_file_path)\n",
    "    except:\n",
    "        logging.error(\"Unexpected error: %s\", str(sys.exc_info()[0]))\n",
    "    finally:\n",
    "        logging.info(\"Finished.\")\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
