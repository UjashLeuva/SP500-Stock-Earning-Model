{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv, json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-019a8f08e73b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mNYTimes_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mcount_total_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_total_articles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNYTimes_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"docs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNYTimes_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"docs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'response'"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "## Preparing DJIA data\n",
    "# Reading DJIA index prices csv file\n",
    "with open('/Users/Ujash/Documents/StockPredictionJPM/GSPC.csv', \"r\") as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    # Converting the csv file reader to a lists \n",
    "    data_list = list(spamreader)\n",
    "\n",
    "# Separating header from the data\n",
    "header = data_list[0] \n",
    "data_list = data_list[1:] \n",
    "\n",
    "data_list = np.asarray(data_list)\n",
    "\n",
    "# Selecting date and close value for each day\n",
    "selected_data = data_list[:, [0, 4, 6]]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=selected_data[0:,1:],\n",
    "             index=selected_data[0:,0],\n",
    "                                columns=['close', 'adj close'],\n",
    "                                        dtype='float64')\n",
    "\n",
    "# Reference for pandas interpolation http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "# Adding missing dates to the dataframe\n",
    "df1 = df\n",
    "idx = pd.date_range('12-29-2006', '12-31-2016')\n",
    "df1.index = pd.DatetimeIndex(df1.index)\n",
    "df1 = df1.reindex(idx, fill_value=np.NaN)\n",
    "# df1.count() # gives 2518 count\n",
    "interpolated_df = df1.interpolate()\n",
    "interpolated_df.count() # gives 3651 count\n",
    "\n",
    "# Removing extra date rows added in data for calculating interpolation\n",
    "interpolated_df = interpolated_df[3:]\n",
    "\n",
    "###############################################################################################  \n",
    "## Preparing NYTimes data\n",
    "# Function to parse and convert date format\n",
    "date_format = [\"%Y-%m-%dT%H:%M:%SZ\", \"%Y-%m-%dT%H:%M:%S+%f\"]\n",
    "def try_parsing_date(text):\n",
    "    for fmt in date_format:\n",
    "        #return datetime.strptime(text, fmt)\n",
    "        try:\n",
    "            return datetime.strptime(text, fmt).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years = [2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007]\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "dict_keys = ['pub_date', 'headline'] #, 'lead_paragraph']\n",
    "articles_dict = dict.fromkeys(dict_keys)\n",
    "# Filtering list for type_of_material\n",
    "type_of_material_list = ['blog', 'brief', 'news', 'editorial', 'op-ed', 'list','analysis']\n",
    "# Filtering list for section_name\n",
    "section_name_list = ['business', 'national', 'world', 'u.s.' , 'politics', 'opinion', 'tech', 'science',  'health']\n",
    "news_desk_list = ['business', 'national', 'world', 'u.s.' , 'politics', 'opinion', 'tech', 'science',  'health', 'foreign']\n",
    "\n",
    "current_date = '2016-01-01'\n",
    "from datetime import datetime\n",
    "#years = [2015]\n",
    "#months = [3]\n",
    "\n",
    "current_article_str = ''      \n",
    "\n",
    "## Adding article column to dataframe\n",
    "interpolated_df[\"articles\"] = ''\n",
    "count_articles_filtered = 0\n",
    "count_total_articles = 0\n",
    "count_main_not_exist = 0               \n",
    "count_unicode_error = 0     \n",
    "count_attribute_error = 0   \n",
    "for year in years:\n",
    "    for month in months:\n",
    "        file_str = '/Users/Ujash/Documents/StockPredictionJPM/NewsData/' + str(year) + '-' + '{:02}'.format(month) + '.json'\n",
    "        with open(file_str) as data_file:    \n",
    "            NYTimes_data = json.load(data_file)\n",
    "        count_total_articles = count_total_articles + len(NYTimes_data[\"response\"][\"docs\"][:])\n",
    "        for i in range(len(NYTimes_data[\"response\"][\"docs\"][:])):\n",
    "            try:\n",
    "                if any(substring in NYTimes_data[\"response\"][\"docs\"][:][i]['type_of_material'].lower() for substring in type_of_material_list):\n",
    "                    if any(substring in NYTimes_data[\"response\"][\"docs\"][:][i]['section_name'].lower() for substring in section_name_list):\n",
    "                        #count += 1\n",
    "                        count_articles_filtered += 1\n",
    "                        #print 'i: ' + str(i)\n",
    "                        articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "                        articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "                        #articles_dict['headline'] = articles_dict['lead_paragraph'] # Selecting lead_paragraph\n",
    "                        date = try_parsing_date(articles_dict['pub_date'])\n",
    "                        #print 'article_dict: ' + articles_dict['headline']\n",
    "                        if date == current_date:\n",
    "                            current_article_str = current_article_str + '. ' + articles_dict['headline']\n",
    "                        else:  \n",
    "                            interpolated_df.set_value(current_date, 'articles', interpolated_df.loc[current_date, 'articles'] + '. ' + current_article_str)\n",
    "                            current_date = date\n",
    "                            #interpolated_df.set_value(date, 'articles', current_article_str)\n",
    "                            #print str(date) + current_article_str\n",
    "                            current_article_str = articles_dict['headline']\n",
    "                        # For last condition in a year\n",
    "                        if (date == current_date) and (i == len(NYTimes_data[\"response\"][\"docs\"][:]) - 1): \n",
    "                            interpolated_df.set_value(date, 'articles', current_article_str)   \n",
    "                        \n",
    "             #Exception for section_name or type_of_material absent\n",
    "            except AttributeError:\n",
    "                #print 'attribute error'\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                count_attribute_error += 1\n",
    "                # If article matches news_desk_list if none section_name found\n",
    "                try:\n",
    "                    if any(substring in NYTimes_data[\"response\"][\"docs\"][:][i]['news_desk'].lower() for substring in news_desk_list):\n",
    "                            #count += 1\n",
    "                            count_articles_filtered += 1\n",
    "                            #print 'i: ' + str(i)\n",
    "                            articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "                            articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "                            #articles_dict['headline'] = articles_dict['lead_paragraph'] # Selecting lead_paragraph\n",
    "                            date = try_parsing_date(articles_dict['pub_date'])\n",
    "                            #print 'article_dict: ' + articles_dict['headline']\n",
    "                            if date == current_date:\n",
    "                                current_article_str = current_article_str + '. ' + articles_dict['headline']\n",
    "                            else:  \n",
    "                                interpolated_df.set_value(current_date, 'articles', interpolated_df.loc[current_date, 'articles'] + '. ' + current_article_str)\n",
    "                                current_date = date\n",
    "                                #interpolated_df.set_value(date, 'articles', current_article_str)\n",
    "                                #print str(date) + current_article_str\n",
    "                                current_article_str = articles_dict['headline']\n",
    "                            # For last condition in a year\n",
    "                            if (date == current_date) and (i == len(NYTimes_data[\"response\"][\"docs\"][:]) - 1): \n",
    "                                interpolated_df.set_value(date, 'articles', current_article_str)   \n",
    "                \n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                pass\n",
    "            except KeyError:\n",
    "                #print \"key error\"\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                count_main_not_exist += 1\n",
    "                pass   \n",
    "            except TypeError:\n",
    "                #print \"type error\"\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                count_main_not_exist += 1\n",
    "                pass\n",
    "         \n",
    "\n",
    "              \n",
    "print (count_articles_filtered) \n",
    "print (count_total_articles)                     \n",
    "print (count_main_not_exist)\n",
    "print (count_unicode_error)\n",
    "\n",
    "\n",
    "\n",
    "## Putting all articles if no section_name or news_desk not found\n",
    "for date, row in interpolated_df.T.iteritems():   \n",
    "    if len(interpolated_df.loc[date, 'articles']) <= 400:\n",
    "        #print interpolated_df.loc[date, 'articles']\n",
    "        #print date\n",
    "        month = date.month\n",
    "        year = date.year\n",
    "        file_str = '/Users/Ujash/Documents/StockPredictionJPM/NewsData/' + str(year) + '-' + '{:02}'.format(month) + '.json'\n",
    "        with open(file_str) as data_file:    \n",
    "            NYTimes_data = json.load(data_file)\n",
    "        count_total_articles = count_total_articles + len(NYTimes_data[\"response\"][\"docs\"][:])\n",
    "        interpolated_df.set_value(date.strftime('%Y-%m-%d'), 'articles', '')\n",
    "        for i in range(len(NYTimes_data[\"response\"][\"docs\"][:])):\n",
    "            try:\n",
    "                \n",
    "                articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "                articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "                #articles_dict['headline'] = articles_dict['lead_paragraph'] # Selecting lead_paragraph       \n",
    "                pub_date = try_parsing_date(articles_dict['pub_date'])\n",
    "                #print 'article_dict: ' + articles_dict['headline']\n",
    "                if date.strftime('%Y-%m-%d') == pub_date: \n",
    "                    interpolated_df.set_value(pub_date, 'articles', interpolated_df.loc[pub_date, 'articles'] + '. ' + articles_dict['headline'])  \n",
    "                \n",
    "            except KeyError:\n",
    "                #print \"key error\"\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                #count_main_not_exist += 1\n",
    "                pass   \n",
    "            except TypeError:\n",
    "                #print \"type error\"\n",
    "                #print NYTimes_data[\"response\"][\"docs\"][:][i]\n",
    "                #count_main_not_exist += 1\n",
    "                pass\n",
    "\n",
    "\n",
    "#>>> print count_articles_filtered \n",
    "#440770\n",
    "#>>> print count_total_articles \n",
    "#1073132\n",
    "\n",
    "\n",
    "## Filtering the whole data for a year\n",
    "#filtered_data = interpolated_df.ix['2016-01-01':'2016-12-31']\n",
    "#filtered_data.to_pickle('/Users/Dinesh/Documents/Project Stock predictions/data/pickled_ten_year_all.pkl')  \n",
    "\n",
    "\n",
    "# Saving the data as pickle file\n",
    "interpolated_df.to_pickle('/Users/Ujash/Documents/StockPredictionJPM/NewsData/pickled_ten_year_filtered_lead_para.pkl')  \n",
    "\n",
    "\n",
    "# Save pandas frame in csv form\n",
    "interpolated_df.to_csv('/Users/Ujash/Documents/StockPredictionJPM/NewsData/sample_interpolated_df_10_years_filtered_lead_para.csv',sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# Reading the data as pickle file\n",
    "dataframe_read = pd.read_pickle('/Users/Ujash/Documents/StockPredictionJPM/NewsData/pickled_ten_year_filtered_lead_para.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "# Filtering rows\n",
    "#filtered_data = interpolated_df.ix['2016-01-01':'2016-12-31']\n",
    "\n",
    "# Syntax for accessing the data\n",
    "#NYTimes_data[\"response\"][\"docs\"][1:2][:]['headline']['main']\n",
    "#NYTimes_data[\"response\"][\"docs\"][1:2][0]['pub_date']\n",
    "     \n",
    "\n",
    "#    articles_dict = { your_key: NYTimes_data[\"response\"][\"docs\"][:][i][your_key] for your_key in dict_keys }\n",
    "#    try:\n",
    "#        articles_dict['headline'] = articles_dict['headline']['main'] # Selecting just 'main' from headline\n",
    "#    except KeyError:\n",
    "#        count_main_not_exist += 1\n",
    "#        pass   \n",
    "#    except TypeError:\n",
    "#        count_main_not_exist += 1\n",
    "#        pass\n",
    "\n",
    "\n",
    "        \n",
    "# Find out articles with less number of articles\n",
    "# for date, row in interpolated_df.T.iteritems():   \n",
    "#     if len(interpolated_df.loc[date, 'articles']) < 300:\n",
    "#         print interpolated_df.loc[date, 'articles']\n",
    "#         print date\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,tzinfo,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API Keys (please use your keys)\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
